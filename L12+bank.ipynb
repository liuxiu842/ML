{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L12+bank.ipynb",
      "provenance": [],
      "mount_file_id": "1216nWoC6HIz2I4_vKDVZBli3edwPGFv-",
      "authorship_tag": "ABX9TyPy0p2cG3Lo//Q1QW2Zr1kh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuxiu842/ML/blob/master/L12%2Bbank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-6MiXIC8qor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import  numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline\n",
        "\n",
        "train_data =pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/L12/train.csv\")\n",
        "test_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/L12/test.csv')\n",
        "\n",
        "\n",
        "train_data.columns.to_list()\n",
        "\n",
        "test_data.columns.to_list()\n",
        "\n",
        "for  feature in train_data.columns:\n",
        "    print ( feature ,\" ==== > \"  ,  train_data[feature].isnull().sum()/len(train_data))\n",
        "\n",
        "for  feature in test_data.columns:\n",
        "    print ( feature ,\" ==== > \"  ,  test_data[feature].isnull().sum()/len(test_data)) \n",
        "\n",
        "train_data.drop( [\"conyuemp\" , \"ult_fec_cli_1t\"] ,axis =1 , inplace= True )\n",
        "test_data.drop( [\"conyuemp\" , \"ult_fec_cli_1t\"] ,axis =1 , inplace= True )\n",
        "\n",
        "train_data.dtypes\n",
        "all_data =  pd.concat( [ train_data , test_data] , axis= 0 )\n",
        "\n",
        "all_data.age.value_counts().index.to_list() \n",
        "\n",
        "categorical_features = all_data.select_dtypes(\"object\")\n",
        "categorical_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tnODC3ixyxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data.age = all_data.age.str.strip().replace(\" \" ,\"\")\n",
        "all_data.loc[all_data.age == 'NA' ,\"age\"].count()\n",
        "all_data.loc[all_data.age == 'NA' ,\"age\"] = '23'\n",
        "all_data.age.fillna( '23' , inplace= True )\n",
        "all_data.antiguedad = all_data.antiguedad.str.strip().replace(\" \" ,\"\")\n",
        "all_data.antiguedad.value_counts()\n",
        "\n",
        "all_data.loc[all_data.antiguedad == 'NA' ,\"antiguedad\"] = '21'\n",
        "all_data.antiguedad.fillna('21' ,inplace= True  )\n",
        "all_data.age = pd.to_numeric(all_data.age).astype('int16')\n",
        "all_data.antiguedad = pd.to_numeric(all_data.antiguedad).astype('int16')\n",
        "categorical_features = all_data.select_dtypes(\"object\").columns\n",
        "categorical_features\n",
        "\n",
        "for  categorical_feature in categorical_features:\n",
        "    print (categorical_feature  ,\"== > \" ,  all_data[categorical_feature].nunique() )\n",
        "\n",
        "all_data.fecha_alta.value_counts()\n",
        "all_data.fecha_alta.fillna(\"2014-07-28\" , inplace= True )\n",
        "all_data.pais_residencia.fillna('ES',inplace=True)\n",
        "for categorical_feature  in categorical_features :\n",
        "    mode= all_data[categorical_feature].value_counts().index[0]\n",
        "    all_data[categorical_feature].fillna( mode , inplace= True )\n",
        "\n",
        "\n",
        "all_data.isna().sum()\n",
        "all_data.ind_nuevo.fillna(0. ,inplace= True)\n",
        "\n",
        "all_data.ind_nuevo = all_data.ind_nuevo.map( {0.0 : 0 , 1.0 :1})\n",
        "\n",
        "all_data.indrel.value_counts()\n",
        "\n",
        "all_data.indrel.fillna(1. ,inplace= True)\n",
        "all_data.indrel = all_data.indrel.map( {1.0 : 0 , 99.0 :1})\n",
        "\n",
        "all_data.drop('tipodom' ,axis =1  ,inplace= True)\n",
        "\n",
        "all_data.drop('cod_prov' ,axis =1  ,inplace= True)\n",
        "\n",
        "all_data.ind_actividad_cliente.value_counts()\n",
        "all_data.ind_actividad_cliente.fillna(0. , inplace= True)\n",
        "all_data.renta.fillna(all_data.renta.mean() ,inplace= True )\n",
        "all_data.isna().sum()\n",
        "\n",
        "all_data.fecha_alta = pd.to_datetime(all_data.fecha_alta)\n",
        "all_data.fecha_alta\n",
        "\n",
        "all_data['fecha_alta_year']  = all_data['fecha_alta'].dt.year\n",
        "all_data['fecha_alta_month']  = all_data['fecha_alta'].dt.month\n",
        "all_data['fecha_alta_day']  = all_data['fecha_alta'].dt.day\n",
        "\n",
        "\n",
        "from datetime import  datetime \n",
        "all_data['fecha_alta_weekday'] = all_data['fecha_alta'].apply(datetime.weekday  )\n",
        "\n",
        "all_data.fecha_dato = pd.to_datetime(all_data.fecha_dato)\n",
        "all_data['fecha_dato_year']  = all_data['fecha_dato'].dt.year\n",
        "all_data['fecha_dato_month']  = all_data['fecha_dato'].dt.month\n",
        "all_data['fecha_dato_day']  = all_data['fecha_dato'].dt.day\n",
        "all_data['fecha_dato_weekday'] = all_data['fecha_dato'].apply(datetime.weekday)\n",
        "\n",
        "all_data.loc[all_data.indrel_1mes == 'P' ,'indrel_1mes'] = '5.0'\n",
        "all_data.indrel_1mes = pd.to_numeric(all_data.indrel_1mes , errors='coerce')\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "encoder = LabelEncoder()\n",
        "for categorical_feature  in categorical_features :\n",
        "    try:\n",
        "        all_data[categorical_feature] = encoder.fit_transform(\n",
        "            all_data[categorical_feature])\n",
        "    except Exception as  e:\n",
        "        print (categorical_feature)\n",
        "        print (e)\n",
        "\n",
        "\n",
        "all_data.dtypes\n",
        "\n",
        "all_data.to_csv(\"processed.csv\" , index= False  )\n",
        "\n",
        "labels = [ \n",
        "    'ind_ahor_fin_ult1',\n",
        "     'ind_aval_fin_ult1',\n",
        "     'ind_cco_fin_ult1',\n",
        "     'ind_cder_fin_ult1',\n",
        "     'ind_cno_fin_ult1',\n",
        "     'ind_ctju_fin_ult1',\n",
        "     'ind_ctma_fin_ult1',\n",
        "     'ind_ctop_fin_ult1',\n",
        "     'ind_ctpp_fin_ult1',\n",
        "     'ind_deco_fin_ult1',\n",
        "     'ind_deme_fin_ult1',\n",
        "     'ind_dela_fin_ult1',\n",
        "     'ind_ecue_fin_ult1',\n",
        "     'ind_fond_fin_ult1',\n",
        "     'ind_hip_fin_ult1',\n",
        "     'ind_plan_fin_ult1',\n",
        "     'ind_pres_fin_ult1',\n",
        "     'ind_reca_fin_ult1',\n",
        "     'ind_tjcr_fin_ult1',\n",
        "     'ind_valo_fin_ult1',\n",
        "     'ind_viv_fin_ult1',\n",
        "     'ind_nomina_ult1',\n",
        "     'ind_nom_pens_ult1',\n",
        "     'ind_recibo_ult1'] \n",
        "\n",
        "features = [  feature  for  feature in all_data.columns.to_list() if feature not in labels ]\n",
        "features\n",
        "\n",
        "columns = all_data.columns.to_list()\n",
        "feature_indices = [columns.index( feature) for feature in features ]\n",
        "label_indices = [columns.index( feature) for feature in labels  ]\n",
        "X_train = all_data.iloc[0: len(train_data) , feature_indices ]\n",
        "y_train  = all_data.iloc[0: len(train_data) , label_indices ]\n",
        "X_test = all_data.iloc[len(train_data): , feature_indices ]\n",
        "\n",
        "X_train.to_csv( 'x-train.csv' , index = False )\n",
        "y_train.to_csv(\"y_train.csv\" , index = False )\n",
        "X_test.to_csv(\"x_test.csv\"  , index = False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEcTBwgxz4We",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import  numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "X_train = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/L12/train.csv\")\n",
        "x_test = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/L12/x_test.csv\")\n",
        "y_train_list  = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/L12/y_train.csv\")\n",
        "y_train_list.fillna( 0.  ,inplace = True)\n",
        "y_train_list= y_train_list.astype(\"int32\")\n",
        "\n",
        "labels = y_train_list.columns.to_list()\n",
        "codepers = X_train.ncodpers.to_list()\n",
        "\n",
        "\n",
        "from collections import defaultdict \n",
        "activate_map =  defaultdict( list )\n",
        "for idx  , row in  y_train_list.iterrows() :\n",
        "    for  j , label in  enumerate(labels) :\n",
        "        if row[label]!=0 :\n",
        "            if   not j  in  activate_map[codepers[idx]]:\n",
        "                activate_map[codepers[idx]].append(j ) # 加入编号\n",
        "        else :\n",
        "            continue"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxxqdp29JRUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activate_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsjXZqBKJLLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time \n",
        "from lightgbm import LGBMClassifier\n",
        "import lightgbm as lgb \n",
        "from  sklearn.model_selection import train_test_split \n",
        "predict_list = [] \n",
        "for label in labels : \n",
        "    print (\"正在训练 ， \" ,label )\n",
        "    start = time.time()\n",
        "    y = y_train_list[label]\n",
        "    x_train ,x_valid ,y_train , y_valid = train_test_split(X_train ,\n",
        "                                                            y,\n",
        "                                                       test_size = 0.2)\n",
        "\n",
        "    params= {'boosting_type':'gbdt',\n",
        "        'objective' : 'binary', #任务类型\n",
        "        'metric' : 'auc', #评估指标\n",
        "        'learning_rate' : 0.01, #学习率\n",
        "        'max_depth' : 10, #树的最大深度\n",
        "        'feature_fraction':0.8, #设置在每次迭代中使用特征的比例\n",
        "        'bagging_fraction': 0.9, #样本采样比例\n",
        "        'bagging_freq': 8, #bagging的次数\n",
        "        'lambda_l1': 0.6, #L1正则\n",
        "        'lambda_l2': 0, #L2正则\n",
        "    }\n",
        "    \n",
        "    train_data = lgb.Dataset(x_train, label=y_train)\n",
        "    valid_data = lgb.Dataset(x_valid, label=y_valid)\n",
        "    model = lgb.train(params,\n",
        "                      train_data,\n",
        "                      valid_sets=[train_data,valid_data],\n",
        "                      num_boost_round =500 ,\n",
        "                      early_stopping_rounds=200,\n",
        "                      verbose_eval=25)\n",
        "    \n",
        "    predict=model.predict(x_test)\n",
        "    predict = pd.Series(predict).map(  lambda x :1 if x> 0.5 else 0 )\n",
        "    predict_list.append(predict.values) \n",
        "    print ( time.time() - start )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Skvowe30La",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict = np.array(  predict_list).T\n",
        "predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjpaXcj133wL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx , line in enumerate(predict):\n",
        "    temp =  np.argwhere(  line ==1 ).ravel()\n",
        "    l = []\n",
        "    if  len(temp) >0 :\n",
        "        for  j in temp :\n",
        "            if j not in activate_map[idx] :\n",
        "                l.append(  labels[j])\n",
        "    to_list.append(l )\n",
        "\n",
        "x_test['added_products']  = pd.Series(to_list).map( lambda x : ' '.join(x))\n",
        "\n",
        "x_test[['ncodpers' ,\"added_products\"]].to_csv('submition.csv' ,index = False)\n",
        "\n",
        "x_test[['ncodpers' ,\"added_products\"]].sort_values(\"ncodpers\").to_csv('submition1.csv' ,index = False)\n",
        "\n",
        "x_test[['ncodpers' ,\"added_products\"]]\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}